---
title: "Extracting elements in R"
author: "Rakesh Poduval, `r Sys.Date()`"
output:
  html_document: 
    toc: FALSE
    toc_depth: 2
    toc_float: TRUE
    theme: yeti
    highlight: monochrome
    number_sections: false
---

```{r setup, include = F}
knitr::opts_chunk$set(echo = T)
```

# Objective
The objective of this document is to get familiar with extracting elements from a R object (object level programming). We can even verify them with the theoretical expectations. Let's start with a `lm` (linear regression model) object.

# Design
Let's use the `mtcars` data set for this purpose.

## Define formula

```{r formula}
form <- mpg ~ cyl + hp + qsec
```

## Prepare data

```{r prepareData}
head(Y <- mtcars$mpg)
head(X <- as.matrix(cbind(Intercept = 1, mtcars[, all.vars(form)[-1]])))
```

# Compare elements

## beta

The least square estimate is defined as : $Î² = (X^t.X)^{-1}.X_t.Y$

```{r beta}
beta <- solve(crossprod(X)) %*% crossprod(X, Y)   # manual beta calculation from theory
t(beta)                                           # from theory

fit <- lm(formula = form, data = mtcars)          # model fit using lm function
coefficients(fit)                                 # from lm object
```

## Predictions

```{r fiitted}
head(fitted <- X %*% beta) # from theory
head(fitted(fit))          # from lm object
```

## Degrees of freedom (df)
degrees of freedom(df) = number of records(n) - number of independent/explanatory variables(k) - 1 
i.e $df = n-k-1$

```{r df}
(df <- (n <- length(Y)) - (k <- ncol(X) - 1) - 1) # from theory
(summary(fit)$df[2])                              # from lm object
```

## Residual standard error (rse) 
It is the estimate of actual residual standard deviation. Residuals are nothing but sum of all actual minus predicted values 
i.e $rse = \sqrt{\sum_{i=1}^{n}(Y - Y')^2/df}$

```{r rse}
(rse <- sqrt(sum(residuals(fit)^2)/df)) # from theory
sigma(fit)                              # from lm object
```

## Standard error of coefficients (se)
The standard error is an estimate of the standard deviation of the coefficient, the amount it varies across cases 
i.e $se = \sqrt{(\sum_{i=1}^{n}(Y - Y')^2/df) * diag((X^T*X)^{-1})}$

```{r se}
(se <- sqrt(rse^2 * diag(solve(crossprod(X))))) # from theory
(summary(fit)$coefficients[, 2])                # from lm object
```

## T and P value 
Under the null hypothesis $H0 : \beta_0 = \beta_1 = \beta_2 = ... = 0$ 
t value is used to determine the significance of the coefficients. It is calculated as $\beta/se$. p value is also used to test the significance of coefficients. Generaly if p value is less than 0.05(confidence level), the coefficient is then considered to be significant.

```{r t}
(tvalue <- t(beta)/se)           # from theory
(summary(fit)$coefficients[, 3]) # from lm object

(pvalue <- 2*(1 - pt(abs(tvalue), df = df))) # from theory
(summary(fit)$coefficients[, 4])             # from lm object
```

## Multiple and adjusted r-squared
It tells us the amount of variation in target variable which is explained by the input variables. It is calculated as follows :
$R^2 = SSR/ SST$ where $SSR = \sum_((Y' - Y^{bar})^2)$ and $SST = SSR + SSE$, $SSE = \sum_{i=1}^n(Y-Y')^2$
Drawback of multiple r squared is that it keeps on increasing as the number of input variable increases even if the additional input variables does not add much(not significant)to the model. Adjusted r squared is a penalized r squared which takes care of this problem.
$adjR^2 = 1 - ((1 - R^2) * ((n-1)/df))$

```{r mr}
SSR <- sum((fitted - mean(Y))^2)
SST <- SSR + (SSE <- sum(residuals(fit)^2))

(Rsquared <- SSR/SST)    # from theory
(summary(fit)$r.squared) # from lm object

(adjRsquared <- 1 - ((1 - Rsquared) * ((n - 1)/df))) # from theory
(summary(fit)$adj.r.squared)                         # from lm object
```

## F-statistic
Under the null hypothesis $H_0 :$ the model is not significant, We use the F test to test the overall significance of the model. The f statistic and correponding its p value helps to justify out claim. It is defined as 
$Fstat  = (SSR/k)/(SSE/df)$ with ($k$, $n - k - 1$) degrees of freedom.

```{r f}
(fstatistic <- (SSR/k)/(SSE/df))                   # from theory
(summary(fit)$fstatistic["value"])                 # from lm object
```

# Remarks
One can test the same with another model and data to confirm.
